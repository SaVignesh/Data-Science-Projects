{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V2ZRzYGsCSnt"
   },
   "source": [
    "\n",
    "### 1. Download the data from <a href='https://drive.google.com/file/d/15dCNcmKskcFVjs7R0ElQkR61Ex53uJpM/view?usp=sharing'>here</a>. You have to use data.csv file for this assignment\n",
    "### 2. Code the model to classify data like below image. You can use any number of units in your Dense layers.\n",
    "\n",
    "<img src='https://i.imgur.com/33ptOFy.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bg22rD7sDPDu"
   },
   "source": [
    "# <font color='red'> <b>3. Writing Callbacks </b> </font>\n",
    "## You have to implement the following callbacks\n",
    "-  Write your own callback function, that has to print the micro F1 score and AUC score after each epoch.Do not use tf.keras.metrics for calculating AUC and F1 score.\n",
    "\n",
    "- Save your model at every epoch if your validation accuracy is improved from previous epoch. \n",
    "\n",
    "- You have to decay learning based on below conditions \n",
    "        Cond1. If your validation accuracy at that epoch is less than previous epoch accuracy, you have to decrese the\n",
    "               learning rate by 10%. \n",
    "        Cond2. For every 3rd epoch, decay your learning rate by 5%.\n",
    "        \n",
    "- If you are getting any NaN values(either weigths or loss) while training, you have to terminate your training. \n",
    "\n",
    "- You have to stop the training if your validation accuracy is not increased in last 2 epochs.\n",
    "\n",
    "- Use tensorboard for every model and analyse your scalar plots and histograms. (you need to upload the screenshots and write the observations for each model for evaluation)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xCL3OGS0DsUa"
   },
   "source": [
    "<pre>\n",
    "<b>Model-1</b>\n",
    "<pre>\n",
    "1. Use tanh as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ZaRHRdEHDzOM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "from tensorflow.keras.initializers import RandomUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, LearningRateScheduler, ReduceLROnPlateau\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "try:\n",
    "    if os.path.isdir(\"logs\"):\n",
    "        shutil.rmtree(\"logs\")\n",
    "    if os.path.isdir(\"model_save\"):\n",
    "        shutil.rmtree(\"model_save\")\n",
    "except:\n",
    "    print('Path not present')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>f1</th>\n",
       "      <th>f2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.450564</td>\n",
       "      <td>1.074305</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.085632</td>\n",
       "      <td>0.967682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.117326</td>\n",
       "      <td>0.971521</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.982179</td>\n",
       "      <td>-0.380408</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.720352</td>\n",
       "      <td>0.955850</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         f1        f2  label\n",
       "0  0.450564  1.074305    0.0\n",
       "1  0.085632  0.967682    0.0\n",
       "2  0.117326  0.971521    1.0\n",
       "3  0.982179 -0.380408    0.0\n",
       "4 -0.720352  0.955850    0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need variables with numpy nd array datatype or tensor datatype to do tensor operations\n",
    "y = data.label.values\n",
    "x = data[['f1','f2']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 0, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = tf.keras.utils.to_categorical(y_train, 2) \n",
    "Y_test = tf.keras.utils.to_categorical(y_test, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 2)\n",
      "(4000, 2)\n",
      "(16000, 2)\n",
      "(4000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class F1_score_auc_score(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data):\n",
    "        self.x_test = validation_data[0]\n",
    "        self.y_test = validation_data[1]\n",
    "        self.log_dir = \"logs/fits/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.additional_metrics= {'f1_score':[],'AUC':[]}\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred= np.argmax(self.model.predict(self.x_test),axis=1)\n",
    "        f1_val = f1_score(y_test, y_pred, average = 'micro')\n",
    "        auc_val = roc_auc_score(y_test, y_pred)\n",
    "        self.additional_metrics['f1_score'].append(f1_val)\n",
    "        self.additional_metrics['AUC'].append(auc_val)\n",
    "        print('F1_score: ',f1_val,'AUC: ',auc_val)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/64806541/performing-np-isnan-on-keras-model-weighta-fails-with-typeerror-ufunc-isnan-n\n",
    "# Most of the code below is copied from Call_Backs_Reference.ipynb notebook\n",
    "\n",
    "class TerminateNaN(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        loss = logs.get('loss')\n",
    "        nan_weights = np.any([np.any(np.isnan(x)) for x in self.model.get_weights()])\n",
    "        inf_weights = np.any([np.any(np.isinf(x)) for x in self.model.get_weights()])\n",
    "        if loss is not None:\n",
    "            if np.isnan(loss) or np.isinf(loss) or nan_weights or inf_weights:\n",
    "                print(\"Invalid loss and terminated at epoch {}\".format(epoch))\n",
    "                self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def changeLearningRate(epoch, learning_rate):\n",
    "    changed = learning_rate\n",
    "    if epoch % 3 == 0:\n",
    "        changed = learning_rate * 0.95\n",
    "    \n",
    "    return changed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_auc = F1_score_auc_score(validation_data = [x_test,Y_test])\n",
    "\n",
    "filepath=\"model_save/weights-{epoch:02d}-{val_accuracy:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath=filepath, monitor='val_accuracy',  verbose=1, save_best_only=True, mode='auto')\n",
    "\n",
    "earlystop = EarlyStopping(monitor='val_accuracy', min_delta=0.01, patience=2, verbose=1)\n",
    "\n",
    "lrschedule = LearningRateScheduler(changeLearningRate, verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.9,\n",
    "                              patience=1, min_lr=0.001)  # SGD has default learning rate of 0.01\n",
    "\n",
    "terminateNAN = TerminateNaN()\n",
    "\n",
    "#log_dir = os.path.join(\"logs\",'fits', datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= (\"logs/fits/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \\\n",
    "                       ,histogram_freq=1,write_graph=True)\n",
    "\n",
    "call_back_list = [f1_auc, lrschedule, reduce_lr, terminateNAN, earlystop, checkpoint, tensorboard_callback]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Model 1 - tanh activation, SGD with momentum optimizer and RandomUniform initializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,302\n",
      "Trainable params: 1,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(2,))\n",
    "initializer = RandomUniform(minval=0, maxval=1, seed=0)\n",
    "layer1 = Dense(4,activation='tanh',kernel_initializer= initializer)(input_layer)\n",
    "layer2 = Dense(8,activation='tanh',kernel_initializer= initializer)(layer1)\n",
    "layer3 = Dense(16,activation='tanh',kernel_initializer= initializer)(layer2)\n",
    "layer4 = Dense(32,activation='tanh',kernel_initializer= initializer)(layer3)\n",
    "layer5 = Dense(16,activation='tanh',kernel_initializer= initializer)(layer4)\n",
    "output = Dense(2,activation='softmax',kernel_initializer= initializer)(layer5)\n",
    "\n",
    "model_one = Model(inputs=input_layer,outputs=output)\n",
    "model_one.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      " 4/32 [==>...........................] - ETA: 1s - loss: 0.6937 - accuracy: 0.5044WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0034s vs `on_train_batch_end` time: 0.0200s). Check your callbacks.\n",
      "32/32 [==============================] - 2s 15ms/step - loss: 0.6932 - accuracy: 0.5083 - val_loss: 0.6933 - val_accuracy: 0.5033\n",
      "F1_score:  0.50325 AUC:  0.5032500000000001\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.50325, saving model to model_save\\weights-01-0.5033.hdf5\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.4984 - val_loss: 0.6935 - val_accuracy: 0.5033\n",
      "F1_score:  0.50325 AUC:  0.50325\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.50325\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.008549999445676804.\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6935 - accuracy: 0.4930 - val_loss: 0.6933 - val_accuracy: 0.4975\n",
      "F1_score:  0.4975 AUC:  0.49749999999999994\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.50325\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db93e85be0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optim_1 = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "\n",
    "model_one.compile(optimizer=optim_1, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_one.fit(x_train,Y_train,epochs=10, validation_data=(x_test,Y_test), batch_size=512, callbacks=[call_back_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-eaaeff7e31174cc1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-eaaeff7e31174cc1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model 2 - relu activation, SGD with momentum optimizer and RandomUniform initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CRpsCx3NEAtx"
   },
   "source": [
    "<pre>\n",
    "<b>Model-2</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use RandomUniform(0,1) as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "h-3tV-KIEFc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,302\n",
      "Trainable params: 1,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer = Input(shape=(2,))\n",
    "initializer = RandomUniform(minval=0, maxval=1, seed=0)\n",
    "layer1 = Dense(4,activation='relu',kernel_initializer=initializer)(input_layer)\n",
    "layer2 = Dense(8,activation='relu',kernel_initializer=initializer)(layer1)\n",
    "layer3 = Dense(16,activation='relu',kernel_initializer=initializer)(layer2)\n",
    "layer4 = Dense(32,activation='relu',kernel_initializer=initializer)(layer3)\n",
    "layer5 = Dense(16,activation='relu',kernel_initializer=initializer)(layer4)\n",
    "output = Dense(2,activation='softmax',kernel_initializer=initializer)(layer5)\n",
    "\n",
    "model_two = Model(inputs=input_layer,outputs=output)\n",
    "model_two.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      " 3/32 [=>............................] - ETA: 9s - loss: 801.0432 - accuracy: 0.5098 WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0058s vs `on_train_batch_end` time: 0.1051s). Check your callbacks.\n",
      "32/32 [==============================] - 1s 32ms/step - loss: 77.5268 - accuracy: 0.5017 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "F1_score:  0.5 AUC:  0.5\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.50325\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6932 - accuracy: 0.4908 - val_loss: 0.6931 - val_accuracy: 0.5000\n",
      "F1_score:  0.5 AUC:  0.5\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.50325\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.008549999445676804.\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6933 - accuracy: 0.5000 - val_loss: 0.6932 - val_accuracy: 0.5000\n",
      "F1_score:  0.5 AUC:  0.5\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.50325\n",
      "Epoch 00003: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1db8dd35af0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= (\"logs/fits/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \\\n",
    "                       ,histogram_freq=1,write_graph=True)\n",
    "call_back_list = [f1_auc, lrschedule, reduce_lr, terminateNAN, earlystop, checkpoint, tensorboard_callback]\n",
    "\n",
    "optim_2 = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model_two.compile(optimizer=optim_2, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_two.fit(x_train,Y_train,epochs=10, validation_data=(x_test,Y_test), batch_size=512, callbacks=[call_back_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2352), started 0:03:02 ago. (Use '!kill 2352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5cbb52a487a7c830\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5cbb52a487a7c830\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Model 3 - relu activation, SGD with momentum optimizer and he_uniform initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1e2VaqfEEDE"
   },
   "source": [
    "<pre>\n",
    "<b>Model-3</b>\n",
    "<pre>\n",
    "1. Use relu as an activation for every layer except output layer.\n",
    "2. use SGD with momentum as optimizer.\n",
    "3. use he_uniform() as initilizer.\n",
    "3. Analyze your output and training process. \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "N2M4q3LYEF_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,302\n",
      "Trainable params: 1,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.HeUniform(seed = 0)\n",
    "\n",
    "input_layer = Input(shape=(2,))\n",
    "layer1 = Dense(4,activation='relu',kernel_initializer=initializer)(input_layer)\n",
    "layer2 = Dense(8,activation='relu',kernel_initializer=initializer)(layer1)\n",
    "layer3 = Dense(16,activation='relu',kernel_initializer=initializer)(layer2)\n",
    "layer4 = Dense(32,activation='relu',kernel_initializer=initializer)(layer3)\n",
    "layer5 = Dense(16,activation='relu',kernel_initializer=initializer)(layer4)\n",
    "output = Dense(2,activation='softmax',kernel_initializer=initializer)(layer5)\n",
    "\n",
    "model_three = Model(inputs=input_layer,outputs=output)\n",
    "model_three.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "WOaQiRbZEGDU",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.009499999787658453.\n",
      " 5/32 [===>..........................] - ETA: 4s - loss: 0.6907 - accuracy: 0.4980WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0148s vs `on_train_batch_end` time: 0.1101s). Check your callbacks.\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.6854 - accuracy: 0.5378 - val_loss: 0.6782 - val_accuracy: 0.5972\n",
      "F1_score:  0.59725 AUC:  0.5972500000000001\n",
      "\n",
      "Epoch 00001: val_accuracy improved from 0.50325 to 0.59725, saving model to model_save\\weights-01-0.5972.hdf5\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6727 - accuracy: 0.6011 - val_loss: 0.6656 - val_accuracy: 0.6050\n",
      "F1_score:  0.605 AUC:  0.605\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.59725 to 0.60500, saving model to model_save\\weights-02-0.6050.hdf5\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.009499999694526196.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6610 - accuracy: 0.6219 - val_loss: 0.6546 - val_accuracy: 0.6277\n",
      "F1_score:  0.62775 AUC:  0.6277499999999999\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.60500 to 0.62775, saving model to model_save\\weights-03-0.6277.hdf5\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.009024999709799886.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6518 - accuracy: 0.6254 - val_loss: 0.6454 - val_accuracy: 0.6457\n",
      "F1_score:  0.64575 AUC:  0.64575\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.62775 to 0.64575, saving model to model_save\\weights-04-0.6457.hdf5\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.009025000035762787.\n",
      "32/32 [==============================] - 0s 5ms/step - loss: 0.6441 - accuracy: 0.6332 - val_loss: 0.6390 - val_accuracy: 0.6495\n",
      "F1_score:  0.6495 AUC:  0.6495\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.64575 to 0.64950, saving model to model_save\\weights-05-0.6495.hdf5\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.009025000035762787.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6372 - accuracy: 0.6401 - val_loss: 0.6334 - val_accuracy: 0.6535\n",
      "F1_score:  0.6535 AUC:  0.6535\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.64950 to 0.65350, saving model to model_save\\weights-06-0.6535.hdf5\n",
      "Epoch 00006: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dc6f249d60>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= (\"logs/fits/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \\\n",
    "                       ,histogram_freq=1,write_graph=True)\n",
    "call_back_list = [f1_auc, lrschedule, reduce_lr, terminateNAN, earlystop, checkpoint, tensorboard_callback]\n",
    "\n",
    "optim_3 = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
    "model_three.compile(optimizer= optim_3, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_three.fit(x_train,Y_train,epochs=10, validation_data=(x_test,Y_test), batch_size=512, callbacks=[call_back_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2352), started 0:04:30 ago. (Use '!kill 2352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1b41c3678eb52b7b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1b41c3678eb52b7b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Model 4 - relu activation, Adam optimizer and he_uniform initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w41Y3TFENCXk"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "<pre>\n",
    "<b>Model-4</b>\n",
    "<pre>\n",
    "1. Try with any values to get better accuracy/f1 score.  \n",
    "</pre>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "4agdXzB-DqOj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 4)                 12        \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 2)                 34        \n",
      "=================================================================\n",
      "Total params: 1,302\n",
      "Trainable params: 1,302\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.keras.initializers.HeUniform(seed = 0)\n",
    "\n",
    "input_layer = Input(shape=(2,))\n",
    "layer1 = Dense(4,activation='relu',kernel_initializer=initializer)(input_layer)\n",
    "layer2 = Dense(8,activation='relu',kernel_initializer=initializer)(layer1)\n",
    "layer3 = Dense(16,activation='relu',kernel_initializer=initializer)(layer2)\n",
    "layer4 = Dense(32,activation='relu',kernel_initializer=initializer)(layer3)\n",
    "layer5 = Dense(16,activation='relu',kernel_initializer=initializer)(layer4)\n",
    "output = Dense(2,activation='softmax',kernel_initializer=initializer)(layer5)\n",
    "\n",
    "model_four = Model(inputs=input_layer,outputs=output)\n",
    "model_four.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: LearningRateScheduler setting learning rate to 0.0009500000451225787.\n",
      " 3/32 [=>............................] - ETA: 10s - loss: 0.6904 - accuracy: 0.4954WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0048s vs `on_train_batch_end` time: 0.1201s). Check your callbacks.\n",
      "32/32 [==============================] - 1s 34ms/step - loss: 0.6843 - accuracy: 0.5264 - val_loss: 0.6771 - val_accuracy: 0.5623\n",
      "F1_score:  0.56225 AUC:  0.5622499999999999\n",
      "\n",
      "Epoch 00001: val_accuracy did not improve from 0.65350\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: LearningRateScheduler setting learning rate to 0.0009500000160187483.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6705 - accuracy: 0.5949 - val_loss: 0.6621 - val_accuracy: 0.6240\n",
      "F1_score:  0.624 AUC:  0.624\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.65350\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: LearningRateScheduler setting learning rate to 0.0009500000160187483.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6558 - accuracy: 0.6292 - val_loss: 0.6468 - val_accuracy: 0.6382\n",
      "F1_score:  0.63825 AUC:  0.63825\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.65350\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: LearningRateScheduler setting learning rate to 0.0009025000152178108.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6419 - accuracy: 0.6384 - val_loss: 0.6353 - val_accuracy: 0.6432\n",
      "F1_score:  0.64325 AUC:  0.64325\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.65350\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: LearningRateScheduler setting learning rate to 0.0009025000035762787.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6308 - accuracy: 0.6428 - val_loss: 0.6278 - val_accuracy: 0.6562\n",
      "F1_score:  0.65625 AUC:  0.65625\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.65350 to 0.65625, saving model to model_save\\weights-05-0.6562.hdf5\n",
      "Epoch 6/10\n",
      "\n",
      "Epoch 00006: LearningRateScheduler setting learning rate to 0.0009025000035762787.\n",
      "32/32 [==============================] - 0s 6ms/step - loss: 0.6211 - accuracy: 0.6514 - val_loss: 0.6159 - val_accuracy: 0.6618\n",
      "F1_score:  0.66175 AUC:  0.66175\n",
      "\n",
      "Epoch 00006: val_accuracy improved from 0.65625 to 0.66175, saving model to model_save\\weights-06-0.6618.hdf5\n",
      "Epoch 7/10\n",
      "\n",
      "Epoch 00007: LearningRateScheduler setting learning rate to 0.0008573750033974647.\n",
      "32/32 [==============================] - 0s 4ms/step - loss: 0.6140 - accuracy: 0.6596 - val_loss: 0.6114 - val_accuracy: 0.6662\n",
      "F1_score:  0.66625 AUC:  0.66625\n",
      "\n",
      "Epoch 00007: val_accuracy improved from 0.66175 to 0.66625, saving model to model_save\\weights-07-0.6662.hdf5\n",
      "Epoch 00007: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dba87a5ca0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir= (\"logs/fits/\"+datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")) \\\n",
    "                       ,histogram_freq=1,write_graph=True)\n",
    "call_back_list = [f1_auc, lrschedule, reduce_lr, terminateNAN, earlystop, checkpoint, tensorboard_callback]\n",
    "\n",
    "optim_4 =  tf.keras.optimizers.Adam()\n",
    "model_four.compile(optimizer= optim_4, loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "model_four.fit(x_train,Y_train,epochs=10, validation_data=(x_test,Y_test), batch_size=512, callbacks=[call_back_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 2352), started 0:08:29 ago. (Use '!kill 2352' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e0556158fc3b352a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e0556158fc3b352a\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aP3-7U_4LhC6"
   },
   "source": [
    "# Note \n",
    "Make sure that you are plotting tensorboard plots either in your notebook or you can try to create a pdf file with all the tensorboard screenshots.Please write your analysis of tensorboard results for each model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VPci2vqWMN2I"
   },
   "source": [
    "1. model_one and model_two that used sgd optimizer and Random uniform initializer stopped training at low values of epoch = 3 as there was no improvement in validation accuracy. These models could have been stuck at a saddle point.\n",
    "2. model_three and model_four which used Relu activation function and he initializer were able to continue training till epoch = 6 and epoch = 7 after which there was no improvement in validation accuracy.\n",
    "3. model_two with relu activation function and Random uniform initializer had the highest initial loss of 77.5268 after 1 epoch. This might be because Random uniform initializer does not work well with relu activation function.\n",
    "4. model_three used relu activation function with he uniform initializer which had a initial loss of 0.6854 after 1 epoch which shows that he initializer works well with relu activation function.\n",
    "5. The accuracy of model_one that used tanh activation function, sgd optimizer and Random uniform initializer started decreasing at epoch = 3.\n",
    "6. model_four using relu activation function, adam optimizer and he uniform initializer had the lowest validation loss of 0.6109, highest validation accuracy of 0.669 out of all 4 models."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Call_Backs_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "TensorFlow-GPU-try",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
